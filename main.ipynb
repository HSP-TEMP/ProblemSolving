{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection, AdamW\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from fewshot_dataset import FewShotDetectionDataset, collate_fn\n",
    "from utils import explore_dataset\n",
    "from iou_utils import compute_iou, generalized_iou_loss, ciou_loss\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    parser.add_argument(\"--training_folder\", default=\"./raw_dataset/DL-RX/train\", type=str, required=False)\n",
    "    parser.add_argument(\"--test_folder\",     default=\"./raw_dataset/DL-RX/test\",  type=str, required=False)\n",
    "\n",
    "    parser.add_argument(\"--pretrained_checkpoint\", default=\"google/owlv2-base-patch16-ensemble\", type=str, required=False)\n",
    "    parser.add_argument(\"--apply_finetuning\", default=True, type=bool, required=False)\n",
    "\n",
    "    parser.add_argument(\"--freezing_method\", default=\"tiny\", choices=[\"simple\", \"tiny\"], type=str, required=False)\n",
    "    parser.add_argument(\"--num_epochs\", default=500, type=int, required=False)\n",
    "    parser.add_argument(\"--batch_size\", default=4,  type=int, required=False)\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4, type=float, required=False)\n",
    "    parser.add_argument(\"--early_stopping_patience\", default=10, type=int, required=False)\n",
    "    parser.add_argument(\"--model_saving_dir\", default=\"./finetuned_record\", type=str, required=False)\n",
    "    parser.add_argument('--result_folder', default='./results', type=str, required=False)\n",
    "    return parser.parse_args(args=[]) # [] Only when using jupyter notebook\n",
    "\n",
    "args = parse_args()\n",
    "for key, value in vars(args).items():\n",
    "    print(f\"{key} => {value}\")\n",
    "\n",
    "if not os.path.exists(args.model_saving_dir):\n",
    "    os.makedirs(args.model_saving_dir)\n",
    "    print(f\"New folder created. => [{os.path.abspath(args.model_saving_dir)}]\")\n",
    "\n",
    "if not os.path.exists(args.result_folder):\n",
    "    os.makedirs(args.result_folder)\n",
    "    print(f\"New folder created. => [{os.path.abspath(args.result_folder)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_dataset(dataset_folder=args.training_folder,\n",
    "#                 verbose=True,\n",
    "#                 visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(args.pretrained_checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(args.pretrained_checkpoint)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.freezing_method == \"simple\":\n",
    "    # Freeze parameters: vision/text encoder is freezed, we fine-tune only detection heads\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"vision\" in name or \"text\" in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            print(f\"Target layer: {name}\")\n",
    "\n",
    "elif args.freezing_method == \"tiny\":\n",
    "    allowed_keywords = [\n",
    "        # \"class_head.logit_scale\",\n",
    "        \"box_head\",\n",
    "        \"objectness_head\"\n",
    "    ]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(keyword in name for keyword in allowed_keywords):\n",
    "            param.requires_grad = True\n",
    "            print(f\"Target layer: {name}\")\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    raise Exception(\"Invalid `freezing method`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FewShotDetectionDataset(dataset_folder=args.training_folder, processor=processor)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "sample = next(iter(dataloader))\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Sample pixel_values: {sample['pixel_values'].shape}\")\n",
    "print(f\"Sample input_ids: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention_mask : {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample targets: {sample['targets']}\") # label and bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.apply_finetuning:\n",
    "    model.to(args.device)\n",
    "    saving_dir = args.model_saving_dir\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stopping_count = 0\n",
    "\n",
    "    # Apply same learning rate for all updated parameters\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move batch data to device\n",
    "            pixel_values   = batch[\"pixel_values\"].to(args.device)\n",
    "            input_ids      = batch[\"input_ids\"].to(args.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(args.device)\n",
    "            \n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            pred_boxes = outputs.pred_boxes[:, 0, :]  # shape: [batch_size, 4]\n",
    "            pred_logits = outputs.logits[:, 0, :]     # shape: [batch_size, num_classes]\n",
    "\n",
    "            # Ground truth: bounding box info\n",
    "            gt_boxes = torch.stack([t[\"boxes\"][0] for t in batch[\"targets\"]]).to(args.device) # shape: [batch_size, 4]\n",
    "\n",
    "            # Single class => ground truth label is always zero\n",
    "            gt_labels = torch.zeros(len(batch[\"targets\"]), dtype=torch.long, device=args.device)\n",
    "\n",
    "            # Loss: We focus on localization (loss_box, loss_giou), as classification is optimized well.\n",
    "            loss_cls  = F.cross_entropy(pred_logits, gt_labels)\n",
    "            \n",
    "            # loss_box  = F.l1_loss(pred_boxes, gt_boxes)\n",
    "            loss_box = F.smooth_l1_loss(pred_boxes, gt_boxes)\n",
    "\n",
    "            # loss_giou = generalized_iou_loss(pred_boxes, gt_boxes)            \n",
    "            loss_ciou = ciou_loss(pred_boxes, gt_boxes)\n",
    "            \n",
    "            lambda_cls  = 0.05\n",
    "            lambda_box  = 0.95\n",
    "            # lambda_ciou = 0.55\n",
    "            loss = (lambda_cls * loss_cls) + (lambda_box * loss_box) # + (lambda_ciou * loss_ciou)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            early_stopping_count = 0\n",
    "            best_loss = avg_loss\n",
    "            print(f\"Best model saved at epoch={epoch + 1}\")\n",
    "            model.save_pretrained(saving_dir)\n",
    "            processor.save_pretrained(saving_dir)\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "        \n",
    "        if early_stopping_count == args.early_stopping_patience:\n",
    "            print(f\"\\nFine-tuning early-stopped at epoch={epoch + 1}\")\n",
    "            break\n",
    "        print()\n",
    "    print(\"OWL-ViT few-shot finetuning finished.\")\n",
    "else:\n",
    "    print(\"No finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = args.test_folder\n",
    "test_image_paths = glob.glob(os.path.join(test_folder, '*.bmp'))\n",
    "test_annotation_paths = glob.glob(os.path.join(test_folder, '*.json'))\n",
    "\n",
    "# Fine-tuned checkpoint or raw model\n",
    "finetuned_checkpoint = args.model_saving_dir if args.apply_finetuning else args.pretrained_checkpoint\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(finetuned_checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(finetuned_checkpoint)\n",
    "\n",
    "device = args.device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "IOU_THRESHOLD = 0.5\n",
    "query_label = 'industrial stabbed defect on metal bearing surface'\n",
    "\n",
    "predictions = []   # {\"image_id\": ..., \"score\": ..., \"box\": ...} (Normalized coordinates, ROI)\n",
    "ground_truths = {} # key: image_id, value: ground truth bbox (Normalized coordinates, ROI)\n",
    "\n",
    "for test_image_path, test_annotation_path in zip(test_image_paths, test_annotation_paths):\n",
    "    image_id = os.path.basename(test_image_path)\n",
    "    full_image = Image.open(test_image_path).convert('RGB')\n",
    "    full_width, full_height = full_image.size\n",
    "    \n",
    "    with open(test_annotation_path, 'r', encoding='utf-8') as f:\n",
    "        annotation = json.load(f)\n",
    "    \n",
    "    roi = annotation['rois'][0]\n",
    "    roi_x = roi[0]\n",
    "    roi_y = roi[1]\n",
    "    roi_width = roi[2]\n",
    "    roi_height = roi[3]\n",
    "    \n",
    "    # Crop ROI\n",
    "    cropped_image = full_image.crop((roi_x, roi_y, roi_x + roi_width, roi_y + roi_height))\n",
    "    crop_width, crop_height = cropped_image.size\n",
    "    \n",
    "    # Extract ground truth bbox\n",
    "    shape = annotation['shapes'][0]\n",
    "    gt_bbox = shape['bbox']  # bbox dict {\"x\":..., \"y\":..., \"width\":..., \"height\":...}\n",
    "    gt_full_box = [gt_bbox[\"x\"],\n",
    "                   gt_bbox[\"y\"], \n",
    "                   gt_bbox[\"x\"] + gt_bbox[\"width\"], \n",
    "                   gt_bbox[\"y\"] + gt_bbox[\"height\"]]\n",
    "    \n",
    "    # Normalize ground truth bbox to ROI coordinates\n",
    "    gt_roi_box = [ (gt_full_box[0] - roi_x) / roi_width,\n",
    "                   (gt_full_box[1] - roi_y) / roi_height,\n",
    "                   (gt_full_box[2] - roi_x) / roi_width,\n",
    "                   (gt_full_box[3] - roi_y) / roi_height ]\n",
    "    ground_truths[image_id] = gt_roi_box  # Single object\n",
    "    \n",
    "    # Inference on cropped image\n",
    "    inputs = processor(text=[query_label], images=cropped_image, return_tensors='pt')\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    input_ids = inputs.get('input_ids')\n",
    "    if input_ids is not None:\n",
    "        input_ids = input_ids.to(device)\n",
    "    attention_mask = inputs.get('attention_mask')\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Single object detection => use first query\n",
    "    pred_box_norm = outputs.pred_boxes[0, 0, :].cpu().numpy()  # Normalized bbox [x_min, y_min, x_max, y_max] (ROI)\n",
    "    pred_logits = outputs.logits[0, 0, :].cpu()\n",
    "    score = torch.softmax(pred_logits, dim=0).numpy()[0]\n",
    "    \n",
    "    predictions.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"score\": score,\n",
    "        \"box\": pred_box_norm\n",
    "    })\n",
    "    \n",
    "    # Visualize: Draw ground truth and predicted bbox on ROI image\n",
    "    vis_image = cropped_image.copy()\n",
    "    draw = ImageDraw.Draw(vis_image)\n",
    "    \n",
    "    # Ground Truth: normalized bbox -> absolute coordinates\n",
    "    gt_abs_box = [ int(gt_roi_box[0] * crop_width),\n",
    "                   int(gt_roi_box[1] * crop_height),\n",
    "                   int(gt_roi_box[2] * crop_width),\n",
    "                   int(gt_roi_box[3] * crop_height) ]\n",
    "    # Ensure proper ordering\n",
    "    gt_xmin, gt_ymin, gt_xmax, gt_ymax = (min(gt_abs_box[0], gt_abs_box[2]),\n",
    "                                          min(gt_abs_box[1], gt_abs_box[3]),\n",
    "                                          max(gt_abs_box[0], gt_abs_box[2]),\n",
    "                                          max(gt_abs_box[1], gt_abs_box[3]))\n",
    "    gt_abs_box = [gt_xmin, gt_ymin, gt_xmax, gt_ymax]\n",
    "    \n",
    "    draw.rectangle(gt_abs_box, outline=(0, 255, 0), width=4)\n",
    "    draw.text((gt_xmin, max(0, gt_ymin - 16)), \"Ground Truth\", fill=(0, 255, 0))\n",
    "    \n",
    "    # Prediction: normalized bbox -> absolute coordinates\n",
    "    pred_abs_box = [ int(pred_box_norm[0] * crop_width),\n",
    "                     int(pred_box_norm[1] * crop_height),\n",
    "                     int(pred_box_norm[2] * crop_width),\n",
    "                     int(pred_box_norm[3] * crop_height) ]\n",
    "    # Ensure proper ordering\n",
    "    pred_xmin, pred_ymin, pred_xmax, pred_ymax = (min(pred_abs_box[0], pred_abs_box[2]),\n",
    "                                                  min(pred_abs_box[1], pred_abs_box[3]),\n",
    "                                                  max(pred_abs_box[0], pred_abs_box[2]),\n",
    "                                                  max(pred_abs_box[1], pred_abs_box[3]))\n",
    "    pred_abs_box = [pred_xmin, pred_ymin, pred_xmax, pred_ymax]\n",
    "    \n",
    "    draw.rectangle(pred_abs_box, outline=(255, 0, 0), width=4)\n",
    "    draw.text((pred_xmin, pred_ymin - 16), f\"Prediction ({score:.4f})\", fill=(255, 0, 0))\n",
    "     \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.title(f\"Prediction vs Ground Truth (Image: {image_id})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./results/{os.path.splitext(image_id)[0]}_result.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Evaluation: Calculate AP (IoU threshold=0.5)\n",
    "predictions = sorted(predictions, key=lambda x: x[\"score\"], reverse=True) \n",
    "\n",
    "TPs = np.zeros(len(predictions))\n",
    "FPs = np.zeros(len(predictions))\n",
    "total_gt = len(ground_truths) \n",
    "\n",
    "detected = {image_id: False for image_id in ground_truths.keys()}\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    image_id = pred[\"image_id\"]\n",
    "    pred_box = pred[\"box\"]\n",
    "    gt_box = ground_truths.get(image_id)\n",
    "    \n",
    "    iou = compute_iou(pred_box, gt_box)\n",
    "    if iou >= IOU_THRESHOLD:\n",
    "        if not detected[image_id]:\n",
    "            TPs[idx] = 1\n",
    "            detected[image_id] = True\n",
    "        else:\n",
    "            FPs[idx] = 1\n",
    "    else:\n",
    "        FPs[idx] = 1\n",
    "\n",
    "cum_TP = np.cumsum(TPs)\n",
    "cum_FP = np.cumsum(FPs)\n",
    "precisions = cum_TP / (cum_TP + cum_FP + 1e-6)\n",
    "recalls = cum_TP / (total_gt + 1e-6)\n",
    "\n",
    "AP = 0.0\n",
    "for i in range(1, len(precisions)):\n",
    "    AP += (recalls[i] - recalls[i-1]) * precisions[i]\n",
    "\n",
    "print(\"===== Evaluation Results =====\")\n",
    "print(f\"Total Ground Truth Boxes: {total_gt}\")\n",
    "print(f\"Average Precision (AP) at IoU >= {IOU_THRESHOLD}: {AP:.4f}\")\n",
    "print(f\"Final Recall: {recalls[-1]:.4f}\")\n",
    "print(\"Test inference finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "problem_solving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
